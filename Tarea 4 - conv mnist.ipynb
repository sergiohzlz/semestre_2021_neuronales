{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador de números en MNIST\n",
    "\n",
    "Ocuparemos una red convolucional para clasificar el dataset MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "from tensorflow.keras.utils  import to_categorical\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Este dataset es un estándar para la clasificación de imágenes. Fue recopilado por Yan LeCun y consta de 60k instancias de entrenamiento y 10 de prueba. El dataset de MNIST lo pueden bajar original de la página de Yan LeCun [acá](http://yann.lecun.com/exdb/mnist/). \n",
    "\n",
    "Viene prácticamente en todas las bibliotecas de aprendizaje de máquina y nosotros lo vamos a usar de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_tr, y_tr), (X_ts, y_ts) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print( X_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 8\n",
    "plt.imshow(X_tr[idx])\n",
    "print(y_tr[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1\n",
    "Conozcan el dataset. Hagan una función que agarre 16 imágenes al azar y las grafique en un arreglo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a convertir nuestras matrices en vectores de dimensión 1. Y como cada imágen va de 0 a 255 y nuestras funciones de activación requieren datos entre 0 y 1, vamos a normalizarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_tr.reshape(X_tr.shape[0], 28, 28, 1)\n",
    "X_test = X_ts.reshape(X_ts.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_tr[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los datos a categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(y_tr, 10)\n",
    "Y_test  = to_categorical(y_ts, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "3 equivale a [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "idx = 30\n",
    "print(y_tr.shape)\n",
    "print(Y_train.shape)\n",
    "print(\"{} equivale a {}\".format(y_tr[idx], Y_train[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pueden ver hubo un cambio en la forma de cada etiqueta de respuesta, en vez de ser un número, paso a ser un vector de tamaño 10 para que podamos usar softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "\n",
    "Vamos a agregar capas de convolución sobre nuestros datos. Las dimensiones de entrada las pueden ver como un parámetro `Convolution2D`\n",
    "\n",
    "```python\n",
    "Convolution2D(... , input_shape=(1,28,28))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(8, kernel_size=(3,3), activation='relu', input_shape=(28,28,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 26, 26, 8)\n"
     ]
    }
   ],
   "source": [
    "print( model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Convolution2D(16, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo pondremos dos capas de parámetro de convolución. Lo que sigue es aplanar la salida del modelo hasta este punto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 2304)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora agregaremos una capa densa, una de dropout y al final la de softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               295040    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 297,578\n",
      "Trainable params: 297,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sólo queda compilarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 - 24s - loss: 0.3996 - accuracy: 0.8814 - val_loss: 0.1067 - val_accuracy: 0.9680\n",
      "Epoch 2/10\n",
      "60000/60000 - 20s - loss: 0.1360 - accuracy: 0.9600 - val_loss: 0.0723 - val_accuracy: 0.9773\n",
      "Epoch 3/10\n",
      "60000/60000 - 19s - loss: 0.1030 - accuracy: 0.9689 - val_loss: 0.0598 - val_accuracy: 0.9805\n",
      "Epoch 4/10\n",
      "60000/60000 - 19s - loss: 0.0879 - accuracy: 0.9729 - val_loss: 0.0497 - val_accuracy: 0.9830\n",
      "Epoch 5/10\n",
      "60000/60000 - 25s - loss: 0.0784 - accuracy: 0.9760 - val_loss: 0.0525 - val_accuracy: 0.9824\n",
      "Epoch 6/10\n",
      "60000/60000 - 24s - loss: 0.0692 - accuracy: 0.9786 - val_loss: 0.0435 - val_accuracy: 0.9857\n",
      "Epoch 7/10\n",
      "60000/60000 - 22s - loss: 0.0634 - accuracy: 0.9804 - val_loss: 0.0435 - val_accuracy: 0.9857\n",
      "Epoch 8/10\n",
      "60000/60000 - 24s - loss: 0.0599 - accuracy: 0.9810 - val_loss: 0.0383 - val_accuracy: 0.9876\n",
      "Epoch 9/10\n",
      "60000/60000 - 24s - loss: 0.0574 - accuracy: 0.9821 - val_loss: 0.0385 - val_accuracy: 0.9880\n",
      "Epoch 10/10\n",
      "60000/60000 - 23s - loss: 0.0514 - accuracy: 0.9840 - val_loss: 0.0407 - val_accuracy: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5f80798d30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, \n",
    "          batch_size=356, epochs=10, verbose=2,\n",
    "          validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora debemos evaluar qué tan bien le fue con el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 350us/sample - loss: 0.0407 - accuracy: 0.9870\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score [0.04069638144471682, 0.987]\n"
     ]
    }
   ],
   "source": [
    "print(\"Score {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('conv_digits.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx=8\n",
    "img = X_test[idx].reshape(1,28,28,1)\n",
    "plt.imshow(img[0].reshape(28,28))\n",
    "print(Y_test[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = model.predict(X_test[idx].reshape(1,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.8939393e-06 3.3918368e-11 8.5745793e-09 1.0583043e-08 6.0524968e-08\n",
      "  9.8384398e-01 1.6090944e-02 6.8644730e-09 4.7956950e-05 1.3126502e-05]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(prediccion)\n",
    "print(np.argmax(np.round(prediccion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "Agranden su conjunto de entrenamiento construyendo una función que haga una de estas cosas\n",
    "1. Agregue ruido a cada imagen\n",
    "2. Que rote la imagen \n",
    "3. Que las invierta \n",
    "Pueden basarse en esta [liga de SciKit-Learn](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3\n",
    "Hagan una red neuronal multicapa y comparen la eficiencia de ambas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
